---
title: 『笔记』统计学习方法
date: 2018-06-30 21:30:40
tags: 技术
mathjax: true
---

前段时间帮成儿翻译的有关机器学习的论文被接受了，翻译过程中感觉相关知识还是有必要系统学习一下的，这就趁热学起来吧。我看的是李航的《统计学习方法》，相关知识点和自己的理解会在博客里进行一下记录。

## 1. 概论

### 1.1 统计学习的特点
 - 研究对象是**数据**
 - 目的是对数据进行**预测**和**分析**
 - 是概率论、统计学、信息论、计算理论、最优化理论、计算机科学的**交叉学科**

### 1.2 学习的定义
Herbert A. Simon对**学习**给出以下定义
> 如果一个系统能够通过执行某个过程改进它的性能，这就是学习

### 1.3 统计学习的对象
统计学习的对象是**数据**。
统计学习关于数据的基本假设是**同类数据具有一定的统计规律性**。

<!-- more -->

### 1.4 统计学习在计算机科学中的定位
计算机科学由三维组成：系统、计算、信息。
统计学习属于**信息**这一维。

### 1.5 统计学习的方法
 - 监督学习
 - 非监督学习
 - 半监督学习
 - 强化学习

### 1.6 监督学习
 - 学习方法
  - 训练数据集
  - 假设空间：要学习的模型所属于的**某个函数的集合**
  - 评价准则
  - 测试数据
 - 基本概念
  - 输入空间：输入所有可能取值的集合
  - 输出空间：输出所有可能取值的集合
  - 通常输出空间远远小于输入空间
  - 特征向量：用于表示每个具体的输入即实例
  - 特征空间：所有特征向量存在的空间，特征空间每一维度对应一个特征
  - 模型都是定义在特征空间上的
 - 解决问题类型
  - 回归问题：输入变量与输出变量均为连续变量的预测问题
  - 分类问题：输出变量为有限个离散变量的预测问题
  - 标注问题：输入变量与输出变量均为变量序列的预测问题
 - 基本假设：输入变量$X$和输出变量$Y$具有联合概率分布

### 1.7 统计学习三要素
$$方法 = 模型 + 策略 + 算法$$

 - 模型
  - 模型的假设空间包括所有可能的**条件概率分布**或**决策函数**
  - 假设空间通常是由一个参数向量决定的函数族，参数向量取值于**参数空间**
 - 策略
  - 损失函数（代价函数）：度量模型一次预测的好坏
  - 风险函数（期望损失）：度量平均意义下模型预测的好坏
  - 经验风险（经验损失）：模型关于训练数据集的平均损失
  - 经验风险最小化：过拟合
  - 结构风险最小化：正则化
 - 算法
  - 解析解
  - 数值计算

### 1.8 模型选择
 - 训练误差 = 经验风险
 - 测试误差：反应了学习方法对未知测试数据集的预测能力
 - 目的：测试误差最小化
 - 正则化
   - 结构风险最小化策略的实现
   - 正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大
   - 奥卡姆剃刀原理
 - 交叉验证
   - 将充足的样本分成三部分
      - 训练集
      - 验证集
      - 测试集
   - 简单交叉验证
   - S折交叉验证
   - 留一交叉验证

### 1.9 泛化能力
 - 模型对未知数据的预测能力
 - 泛化误差：期望风险
 - 泛化误差上界
   - 当样本容量增加，泛化误差上界趋于0
   - 假设空间容量越大，模型就越难学，泛化误差上界就越大
 - 泛化误差上界的证明
   - 说明了泛化误差存在上界
   - 从而说明了统计学习的可行性，**证明了机器是可以学习的**

### 1.10 监督学习方法
 - 生成方法
   - 所学到的模型称为生成模型
   - 可以还原出输入和输出的**联合概率分布**
 - 判别方法
   - 所学到的模型称为判别模型
   - 直接学习的是**条件概率分布**或**决策函数**

### 1.11 分类问题
 - 输入变量**可离散，可连续**
 - 输出变量为有限个**离散**变量
 - 分类器性能指标：分类准确率
 - 二类分类器性能指标：精确率、召回率

### 1.12 标注问题
 - 结构预测问题的简单形式
 - 输入是一个**观测序列**
 - 输出是一个**标记序列**或**状态序列**
 - 两个过程：**学习**、**标注**

### 1.13 回归问题
 - 等价于函数拟合
 - 按照输入变量的个数可分为
   - 一元回归
   - 多元回归
 - 按照输入变量与输出变量的关系可分为
   - 线性回归
   - 非线性回归
 - 最常用的损失函数：平方损失函数
     
## 2 感知机
 - **二类**分类的线性分类模型
 - 感知机是神经网络与SVM的基础

### 2.1 感知机模型
$$f(x) = sign(w·x + b)$$

 - $w$叫做**权值**或**权值向量**
 - $b$叫做**偏置**
 - $sign(x) = \begin{cases}+1 & x\geq0\\\\
 -1 & x < 0\end{cases}$
 - 属于判别模型

### 2.2 感知机学习策略
 - 目标是能求得一个能将训练集正实例点和负实例点完全正确分开的分离超平面
 - 学习策略：定义损失函数并将其极小化
 - 损失函数：误分类点到超平面的总距离

### 2.3 感知机学习算法
 - 最优化方法是**随机梯度下降法**
 - **学习率**
 - 感知机学习算法由于采用不同的初值或选取不同的误分类点，解可以不同
 - 原始形式
 - 对偶形式：提高学习效率
 
## 3 $k$近邻法

### 3.1 $k$近邻算法
 - 没有显式的学习过程
 - 利用训练数据集对特征向量空间进行划分
 - 基本要素
   - $k$值的选择
   - 距离度量
   - 分类决策规则
 - 当$k=1$时，$k$近邻算法称为**最近邻算法**

### 3.2 $k$近邻模型
 - 每个训练实例点拥有一个单元
 - 单元：特征空间中，对每个训练实例点，距离该点比其他点更近的所有点组成的一个区域
 - 距离度量
   - $L_p$距离
$$L_p(x_i, x_j)=(\sum_{l=1}^n |x_i^{(l)} - x_j^{(l)}|^p)^{\frac{1}{p}}$$
   - 当$p=2$时，称为欧氏距离
   - 当$p=1$时，称为曼哈顿距离
   - 有不同距离度量所确定的最近邻点是不同的

### 3.3 $k$值的选择
 - 近似误差：模型对训练集的误差
 - 估计误差：模型对测试集的误差
 - $k$值的减小意味着整体模型变得复杂，容易发生过拟合
 - $k$值的增大意味着整体的模型变得简单

### 3.4 分类决策规则
 - **多数表决**

### 3.5 $k$近邻算法的实现
 - 线性扫描
 - kd树：提高$k$近邻搜索的效率